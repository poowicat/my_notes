# 目标

- 数据拆分：训练数据集&测试数据集
- 评价分类结果： 精准度，混淆矩阵，精准率，召回率，F1 Score ，ROC曲线，AUC值等
- 评价回归结果：MSE，RMSE，MAE，R  Squared，调整R Squared

# 训练&测试数据集

问题：训练好的模型能直接拿到生产环境中正确使用吗？

- 我们只是拿到预测的结果，不知道这个模型效果怎么样，准不准？
- 如果拿到真实环境，其实是没有label的，我们怎么对结果进行验证呢？

实际上：从训练好模型到真实使用，好早着呢！我们要做的第一步是：

- 將原始數據中一部分作爲训练数据，另一部分作为测试数据。
- 使用训练数据训练模型，再使用测试数据看好坏。
- 通过测试数据判断模型的好坏
- 然后再对模型不断地修改

# 评价分类结果

## 混淆矩阵

- 对于二分类来说，所有问题被分为0,1两类。混淆举证是2*2的矩阵

## 准确率

- 函数计算分类准确率，返回正确分类的样本比例或者是数量
- 在多标签分类问题中，该函数返回子集的准确率，对于一个给定的多标签样本，如果预测得到的标签集合与该样本真正标签集合严格吻合，则subset accuracy = 1.0 否则是0.0



## 查准率和查全率

- 等指标对衡量机器学习的模型性能在某些场合下要比accuracy更好



## 思考

### 1.是否只关注accuracy？

- accuracy只能判断总的正确率
- 但 存在样本不均衡的情况下，就不能使用accuracy来进行衡量了
- 若：一个总样本 ，正占90%，负占10%，那么只需要将所有样本预测为正，就可以拿到90%的准确率，这显然无意义。
- 正因如此，我们需要precision和recall了

### 2.那么precision和recall怎么取舍？对一个模型来说

- precision应用场景：预测癌症。医生预测为癌症，患者确实患癌症的比例

- recall应用场景：网贷违约率，相对好用户，我们更关心坏用户。

  召回率越高，代表实际坏用户中被预测出来的概率越高

- 至于两个指标如何使用，需要看具体场景，实际上这两个指标是互斥的，一个高，必有另一个低。（埋坑警告）

# 解决样本不均衡问题

## F1 Score

- F1 Score = 2P*R/(P+R)
- P为查准率，R为查全率
- F1 Score就是在P和R中找一个平衡点

## ROC曲线

- 横轴假正率（FP率），纵轴为真正率（TP率）
- 再次把混淆矩阵拿出来，可求得指标（TN,FP,FN,TP)
- TP率 = 召回率 = 灵敏度 = TP/（TP + FN）
- FP率 = FP/(FP + TN)
- TP率与FP率分别从实际正样本，实际负样本的角度来观察概率问题
- 这样就可以解决样本不均衡的问题了
- 例：如上样本不均衡例子，比如，一个90%正样本的总样本中，只用准确率度量是没有意义的。
  - 但如果引入TP率，就只关注90%的正样本
  - 引入FP率，就只关注10%的负样本
  - 如此一来，正负样本不均衡的问题就可以得到解决了。
- roc曲线是通过遍所有阈值来描述整条曲线的
- 如果不断遍历所有阈值，预测的正样本与负样本数就在不断变化，相应的就会在ROC曲线移动
- 注意：ROC曲线本身形状不会变化

## 思考

改变阈值并不会影响roc曲线本身，那么如何评价roc的好坏？

- 回归到曲线本身，横轴是假正率，纵轴是真正率
- 我们希望在FP率低的情况下，TP率高。
- 也就是：ROC曲线越陡越好！



## AUC值

ROC曲线与横轴之间围成的面积

![img](https://pic2.zhimg.com/v2-d556c949ac4b94269f6173cdc833b231_b.jpg)

