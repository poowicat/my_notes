### 1.张量运算的导数：梯度

- 梯度（gradient）是张量运算的导数。它是导数这一概念向多元函数导数推广。多元函数是以张量作为输入的函数。
- 假设有一个输入向量x、一个矩阵w、一个目标y和一个损失函数loss。可以用w来计算预测y_pred，然后计算损失函数或者说预测值y_pred和目标y之间的距离
- y_pred = dot（W，x）
- loss_value = loss（y_pred， y）
- 如果输入数据x和y保持不变，那么这可以看做将W映射到损失的函数
- loss_value = f(W)
- 假设W的当前值为W0，f在W0点的导数是一个张量gradient(f)(W0)，其中形状与W相同，每个系数gradient(f)(W0)[i, j]表示改变W0[i, j]时loss_value变化的方向和大小；

### 2.梯度下降

1. 什么是梯度下降？

   - 是一种思想，没有严格定义，所以用一个比喻来解释什么是梯度下降。

   - 简单来说，梯度下降就是从山顶找一条最短的路走到山脚最低的地方。
   - 但是因为选择方向的原因，我们找到的最低点可能不是真正的最低点。
   - 如图所示，黑线标注的路线所指的方向并不是真正的地方。
   - ![img](../%E5%9B%BE%E7%89%87/782dabe0ce0e156b30ac382ff974a06a.png)
   -  实际上梯度下降就是**随机选择一个方向，然后每次迈步都选择最陡的方向，直到这个方向上能达到的最低点。**

2. 梯度下降用来做什么的？

   - 在求解机器学习参数的优化算法中，使用较多的就是基于梯度下降的优化算法（Gradient Descent， GD）

3. 优缺点

   - 优点：
     1. 效率。在梯度下降法的求解过程中，只需要求解损失函数的一阶导数，计算的代价比较小，可以在很多大规模数据集上用。
   - 缺点：
     1. 求解的是局部最优值，即由于方向选择的问题，得到的结果不一定是全局最优步长选择，过小使得函数收敛速度慢，过大又容易找不到最优解。

4. 梯度下降的变型形式

   根据处理的训练数据不同，主要有以下三种形式：

   1. 批量梯度下降法BGD（Batch Gradient Dscent）
      - 针对的是整个数据集，通过对所有的样本计算来求解梯度方向
      - 优点
        1. 全局最优解
        2. 易于并行实现
      - 缺点
        1. 当数据样本多时，计算量开销大，计算速度慢
   2. 小批量梯度下降法MBGD（mini-batch Gradient Descent）
      - 把数据分为若干批，按批来更新参数，这样一批中的一组数据共同定义了本次梯度的方法，下降起来就容易跑偏，减少了随机性
   3. 随机梯度下降法SGD（stochastic gradient descent）
      - 每个数据都计算以下损失函数，然后求梯度更新参数
      - 优点：计算速度块
      - 缺点： 收敛性能不好
   4. **总结**：SGD可以看做是MBGD的一个特例，及batch_size = 1的情况，在深度学习及机器学习中，基本上都是使用的MBGD算法。

### 3.随机梯度下降（SGD）

- 随机下降法是深度学习最常见的优化方法；
- 目的：求出**最小损失函数**对应的**所有权重值**
- 如图所示，为学习率选择一个合适的值很重要；
- 如果**学习率太小**，则沿着曲线下降需要**迭代很多次**，而且会陷入**局部**极限值;
- 如果**学习率太大**，则更新权重值后可能会出现曲**线完全随机的位置**，将权重w沿着梯度的反方向更新权重，**损失每次都会变小一点**。
- ![img](../%E5%9B%BE%E7%89%87/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAVGFubHlu,size_20,color_FFFFFF,t_70,g_se,x_16-16431828328748.png)

![img](../%E5%9B%BE%E7%89%87/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAVGFubHlu,size_20,color_FFFFFF,t_70,g_se,x_16-164318284013310.png)



- 代码实现：

- BGD批量梯度下降法：**每次迭代使用所有的样本**

- 每次迭代都需要把所有样本都送入，这样的好处是每次迭代都顾及了全部的样本。做的是全局最优化

- ```python
  #-*- coding: utf-8 -*-
  import random
  #用y = Θ1*x1 + Θ2*x2来拟合下面的输入和输出
  #input1  1   2   5   4
  #input2  4   5   1   2
  #output  19  26  19  20
  input_x = [[1,4], [2,5], [5,1], [4,2]]  #输入
  y = [19,26,19,20]   #输出
  theta = [1,1]       #θ参数初始化
  loss = 10           #loss先定义一个数，为了进入循环迭代
  step_size = 0.01    #步长
  eps =0.0001         #精度要求
  max_iters = 10000   #最大迭代次数
  error =0            #损失值
  iter_count = 0      #当前迭代次数
   
  err1=[0,0,0,0]      #求Θ1梯度的中间变量1
  err2=[0,0,0,0]      #求Θ2梯度的中间变量2
   
  while( loss > eps and iter_count < max_iters):   #迭代条件
      loss = 0
      err1sum = 0
      err2sum = 0
      for i in range (4):     #每次迭代所有的样本都进行训练
          pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1]  #预测值
          err1[i]=(pred_y-y[i])*input_x[i][0]
          err1sum=err1sum+err1[i]
          err2[i]=(pred_y-y[i])*input_x[i][1]
          err2sum=err2sum+err2[i]
      theta[0] = theta[0] - step_size * err1sum/4  #对应5式
      theta[1] = theta[1] - step_size * err2sum/4  #对应5式
      for i in range (4):
          pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1]   #预测值
          error = (1/(2*4))*(pred_y - y[i])**2  #损失值
          loss = loss + error  #总损失值
      iter_count += 1
      print ("iters_count", iter_count)
  print ('theta: ',theta )
  print ('final loss: ', loss)
  print ('iters: ', iter_count)
  ```

- 输出：

  ![image-20220126163430299](../%E5%9B%BE%E7%89%87/image-20220126163430299.png)

