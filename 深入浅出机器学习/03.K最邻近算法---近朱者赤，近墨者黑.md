### K最邻近算法原理

- 实际上K这字母的含义就是最邻近的个数
- 在scikit-learn中，K最邻近算法的K值是通过n_neighbors参数来调节的，默认值为5

**注意**：

- K最邻近算法可以用于回归，原理和其用于分类相同；
- 当我们使用K最邻近回归计算某个数据点的预测值时，模型就会**选择**离该数据点**最近的若干**个训练数据集中的点
- 并将它们的y值取**平均值**，并把该平均值**作为新数据点的预测值**。



### K最邻近算法的用法

本节用Jupyter notebook一起实验~

#### K最邻近算法在分类任务中的应用

- 在scikit-learn中，内置若干玩具数据集，还有些API让我们可以自己动手生成一些数据集

- 接下来我们用生成数据集方式来进行展示，请在jupyter notebook上输入以下代码：

  ```python
  # 导入数据集生成器
  from sklearn.datasets import make_blobs
  # 导入KNN分类器
  from sklearn.neighbors import KNeighborsClassifier
  # 导入画图工具
  import matplotlib.pyplot as plt
  # 导入数据集拆分工具
  from sklearn.model_selection import train_test_split
  import numpy as np
  
  # 生成样本数据为200，分类为2的数据
  data = make_blobs(n_samples=200, centers=2, random_state=8)
  x, y = data
  plt.scatter(x[:, 0], x[:, 1], c=y, cmap=plt.cm.spring, edgecolor='k')
  plt.show()
  
  
  # 下面使用L最邻近算法来拟合这些数据
  clf = KNeighborsClassifier()
  clf.fit(x, y)
  # 下面的代码用于画图
  x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1
  y_min, y_max = x[:, 0].min() - 1, x[:, 0].max() + 1
  xx, yy = np.meshgrid(np.arange(x_min, y_max, .02),
                       np.arange(y_min, y_max, .02))
  z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
  z = z.reshape(xx.shape)
  plt.pcolormesh(xx, yy, z, cmap=plt.cm.Pastel1)
  plt.scatter(x[:, 0], x[:, 1], c=y, cmap=plt.cm.spring, edgecolor='k')
  plt.xlim(xx.min(), xx.max())
  plt.ylim(yy.min(), yy.max())
  plt.title("Classifiter: KNN!")
  plt.show()
  ```

- **结果如下：**

![image-20220214103747626](../%E5%9B%BE%E7%89%87/image-20220214103747626.png)

![](../%E5%9B%BE%E7%89%87/output.png)

- **结果分析**：

  



