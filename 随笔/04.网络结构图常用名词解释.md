### Focus：

- 在Yolov5中首次提出

- 具体操作：1pic上每隔一个像素拿一个值——获得四个独立的特征层——将4个特征层进行堆叠——宽高维度转换到通道维度（通道维度都扩增4倍）——通过卷积提取不同的特征

- 提速，减少参数计算，减少cuda使用内存

- 本质：对于图片做切片操作

- 例子：拿yolov5s举例，原始的640 * 640 * 3的图像输入Focus结构后，先切片然后堆叠变成320 * 320 * 12的特征图，再经过一次卷积操作，最终变成320 * 320 * 32的特征图，代码实现如下:

  ```Python
  class Focus(nn.Module):
      # Focus wh information into c-space
      def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups
          super(Focus, self).__init__()
          self.conv = Conv(c1 * 4, c2, k, s, p, g, act)      # 这里输入通道变成了4倍
  
      def forward(self, x):  # x(b,c,w,h) -> y(b,4c,w/2,h/2)
          return self.conv(torch.cat([x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]], 1))
  
  ```

  

### CBL：

- | **(conv+BN+Leaky relu）** | CBL为卷积块：由Conv，Batch Normalization，Leaky relu 这三个网络层组成。 |
  | ------------------------- | ------------------------------------------------------------ |
  |                           |                                                              |

### Bottleneck模块

- 先将channel减小再扩大
- shortcut参数控制是否进行残差链接（ResNet）
- 在yolov5的backbone中的Bottleneck都默认使shortcut为True，在head中的Bottleneck都不使用shortcut。
- 与ResNet对应的是，使用add而非concat进行特征融合。使得融合后的特征数不变

### CSP1_1、CSP1_3...

- 主要从网络结构设计的角度解决推理中计算量很大的问题。
- CSPNet的作者认为推理计算过高的问题是由于网络优化中的梯度信息重复导致的。
- 采用CSP模块先将基础层的特征映射划分为两部分，然后通过跨阶段层次结构将它们合并，在减少了计算量的同时，可以保证准确率。

### C3模块：

- 在新版yolov5中，作者将BottleneckCSP（瓶颈层）模块转变成C3模块

- 其结构作用基本相同均为CSP架构。

- 只是在修正单元的选择上有所不同，其包含3个标准卷积层以及多个Bottleneck模块（数量由配置文件.yaml的n和depth_multiple参数乘积所决定）

- C3相对于BottleneckCSP模块不同的是：经历残差输出后，CONV模块被去掉了，concat后的标准卷积模块总的激活函数也有LeakyRelu变成了SiLU

- 该模块是对残差特征进行学习的主要模块其结构分为2支

- 一支使用上述指定多个Bottleneck堆叠和3个标准卷积层

- 另一只仅经过一个基本卷积模块；

- 最后将两只进行Concat操作

- ```Python
  class C3(nn.Module):
      # CSP Bottleneck with 3 convolutions
      def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion
          super(C3, self).__init__()
          c_ = int(c2 * e)  # hidden channels
          self.cv1 = Conv(c1, c_, 1, 1)
          self.cv2 = Conv(c1, c_, 1, 1)
          self.cv3 = Conv(2 * c_, c2, 1)  # act=FReLU(c2)
          self.m = nn.Sequential(*[Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)])
  
      def forward(self, x):
          return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), dim=1))
  
  
  ```

  

### SPP：

- 是金字塔池化的简称
- 先通过一个标准卷积模块将输入通道减半，然后分别做kernel-size为5,9,13的maxpooling（对于不同核大小，padding是自适应的）

### 上采样：

- 原理: 图像放大几乎都是采用**内插值**方法，即在原有图像的基础上在各个像素之间采用合适的插值算法插入**新**的元素。

### 下采样：

- 原理：缩小图像。

### CONV：

- 为卷积层

- 对输入图像采用多个不同的卷积核进行处理

- 得到不同的响应特征图

- ```Python
  class Conv(nn.Module):
      # Standard convolution
      # ch_in, ch_out, kernel, stride, padding, groups
      def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):
      	# k为卷积核大小，s为步长
      	# g即group,当g=1时，相当于普通卷积,当g>1时,进行分组卷积。
      	# 分组卷积相对与普通卷积减少了参数量，提高训练效率
          super(Conv, self).__init__()
          self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)
          self.bn = nn.BatchNorm2d(c2)
          self.act = nn.Hardswish() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())
   
      def forward(self, x):
          return self.act(self.bn(self.conv(x)))
   
      def fuseforward(self, x):
          return self.act(self.conv(x))
  
  
  ```

  

### slice：



### BN：

- 为批归一化层
- 为神经网络中的一层
- 放在激活函数之前欠，卷积层之后使用
- 得到的特征图个数为m，大小为w*h（即图像像素点个数）
- BN的数据量为m* w *h
- 运算步骤： 所有批处理数据求均值和方差——像素值与均值求差之后除以方差进行规范化——同时加入偏移因子与尺度变化因子控制归一化的值——因子的值由神经网络在训练中学习得到的

### Res unit(残差组件)

### Res（残差块）

- 

- | Res 为残差块：包含两个卷积块和一个add层。 | add层只是将相同维度的张量进行相加,残差这种结构能够保证网络结 |
  | ----------------------------------------- | ------------------------------------------------------------ |
  |                                           |                                                              |

### Maxpool

- 



## 常见激活函数：

### sigmoid：

- [激活函数](https://so.csdn.net/so/search?q=激活函数&spm=1001.2101.3001.7020)在我们的网络模型中比较常用，也常作为二分类任务的输出层，函数的输出范围为（0 ,1）

- ![image-20220426111201531](../../%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/image-20220426111201531.png)

- 优点：

  ​	平滑、易于求导

- 缺点：

  1. 会有梯度消失
  2. 函数不是关于原点对称
  3. 计算exp比较费时

### Relu：

- ReLU激活函数中文名：修正线性单元函数
- ![image-20220426111316503](../../%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/image-20220426111316503.png)
- 优点：
  1. 解决了梯度消失问题，收敛**快于**Sigmoid和tanh，但要**防范ReLU的梯度爆炸**；
  2. 相比Sigmoid和tanh，ReLU计算简单，**提高了运算速度**；
  3. 容易得到更好的模型。
- 缺点：
  1. 输入负数时，ReLU输出总是0，神经元不被激活。

#### **ReLU函数的变型**

![img](../%E5%9B%BE%E7%89%87/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI1MDU2MTc=,size_16,color_FFFFFF,t_70.png)

##### Leakey relu：

- 是Relu函数的变体；
- 解决了Relu函数在进入负区间后，导致神经元不学习的问题；
- ![img](../../%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3l0eV8zMTE=,size_16,color_FFFFFF,t_70.png)

##### PReLU

- 函数中a作为一个可学习的参数，会在训练过程中更新

### Mish

- 表达式：![image-20220426110548306](../%E5%9B%BE%E7%89%87/image-20220426110548306.png)

- ```Python
  class Mish(nn.Module):
      def __init__(self):
          super(Mish, self).__init__()
  
      def forward(self, x):
          return x * torch.tanh(F.softplus(x))
  
  ```

  

### 